{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining template\n",
    "\n",
    "This notebook includes Python code for downloading scanned text of borderlands newspapers and performing word frequency text analyses on the newspapers.\n",
    "\n",
    "To start, follow directions in the [Setup](#Setup) section, below. Once you know which data you would like to use, there are several options listed below for different text analyses.\n",
    "\n",
    "The work is part of the project _Using Newspapers as Data for Collaborative Pedagogy: A Multidisciplinary Interrogation of the Borderlands in Undergraduate Classrooms_, funded in part by the Mellon Foundation through the [Collections as Data](https://collectionsasdata.github.io/part2whole/) program. More information about the project is available found at [https://libguides.library.arizona.edu/newspapers-as-data](https://libguides.library.arizona.edu/newspapers-as-data).\n",
    "\n",
    "If you are not familiar with text data mining, take a look at this nice [StoryMap](https://storymaps.arcgis.com/stories/cd7e273c42cd4ab6b6ce3fa89c13132c) that introduces the idea of text data mining and what we can do with it.\n",
    "\n",
    "This notebook and additional text mining lessons are available at [https://github.com/jcoliver/dig-coll-borderlands](https://github.com/jcoliver/dig-coll-borderlands)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The first decision to make is whether you want to use a small, sample data set or the larger set of data. The latter option requires the files to be downloaded and can take a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you _do not_ want to use the larger set of scanned text, you can use the data that are distributed with this notebook. Running the code block below will show you the data that are available if you do not want to download the larger data set (you do not need to take any extra steps to use the data below, they come with this Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to display table with newspaper information\n",
    "import pandas\n",
    "titles = pandas.read_csv('data/sample/sample-titles.csv')\n",
    "datapath = 'data/sample/'\n",
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to use the entire suite of scanned borderlands newspapers, you will need to first download the files from the University of Arizona Data Repository. Executing the code block below will do this for you (if you just want to try things out with a smaller data set, do not run this block and just jump ahead). Note the data are contained in an archive around 1.5GB and include hundreds of thousands of files. Both the downloading and file extraction steps may take a little while (5 minutes? 10?), so now might be a good time to refill your beverage. When the download and extraction process is complete, a table showing the available data will be printed below the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries necessary for download & extraction\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "# Location of the file on the UA Data Repository\n",
    "url = 'https://arizona.figshare.com/ndownloader/files/31104157'\n",
    "\n",
    "# Download the file & write it to disk\n",
    "zip_filename = 'fulldata.zip'\n",
    "download = urlretrieve(url, zip_filename)\n",
    "\n",
    "# Set the destination for the data files\n",
    "destination = 'data/complete/'\n",
    "\n",
    "# Make sure the destination directory exists\n",
    "if(not(os.path.isdir(destination))):\n",
    "    os.makedirs(destination)\n",
    "\n",
    "# Extract files to destination directory\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zipdata:\n",
    "    zipdata.extractall(destination)\n",
    "    \n",
    "# No need for that zipfile, so we can remove it\n",
    "os.remove(zip_filename)\n",
    "\n",
    "# Finally, display the available titles for this full data set\n",
    "full_titles = pandas.read_csv('data/complete/complete-titles.csv')\n",
    "datapath = 'data/complete/'\n",
    "display(full_titles.sort_values(by=['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency analyses\n",
    "\n",
    "For the newspaper(s) of choice, there are a variety of analyses that can be performed with code below.\n",
    "\n",
    "+ [Investigate word frequency over time for a single word (or set of words) in a single newspaper](#Single-word-in-one-newspaper)\n",
    "+ [Investigate word frequency over time in a pair of newspapers](#Single-word-in-two-newspapers)\n",
    "+ [Investigate frequencies of two words over time for a single newspaper](#Two-words-in-one-newspaper)\n",
    "\n",
    "In all analyses, the code below has example values for newspapers, words, and dates. You can change these as necessary for your specific question.\n",
    "\n",
    "Before you get started though, be sure to run the code block immediately below, which loads in all the libraries necessary for subsequent text data analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to change anything, just run this block of code to load necessary libraries\n",
    "\n",
    "# for data tables\n",
    "import pandas\n",
    "\n",
    "# for file navigation\n",
    "import os\n",
    "\n",
    "# for pattern matching in filenames\n",
    "import re\n",
    "\n",
    "# for text data mining\n",
    "import nltk\n",
    "\n",
    "# for stopword corpora for a variety of languages\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for splitting data into individual words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# for automated text cleaning\n",
    "import digcol as dc\n",
    "\n",
    "# download the stopwords for several languages\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# for drawing the plot\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single word in one newspaper\n",
    "\n",
    "The code below is designed to analyze on set of words for an individual newspaper title. As written, the code block will look at the frequency of influenza-related words (\"flu\", \"influenza\") in _The Bisbee Daily Review_ during the years 1917 and 1918.\n",
    "\n",
    "You can edit the values for `title`, `year_list`, `my_words`, and `language` to fit your analysis of interest. For the value of `title`, be sure to use the value in the \"directory\" column in the table above that corresponds to the newspaper of interest. For example, if you wanted to look at _El Tucsonense_, change this:\n",
    "\n",
    "`title = 'bisbee-daily-review'`\n",
    "\n",
    "to this:\n",
    "\n",
    "`title = 'el-tucsonense'`\n",
    "\n",
    "For `year_list`, list all the years of interest, each enclosed in single quotes (') and values separated by a comma. If you are only interested in one year, no comma is necessary.\n",
    "\n",
    "The words listed in `my_words` will effectively be \"lumped together\" - that is, for this example, the plot will show the frequency of 'flu' and 'influenza' combined. If you are interested in plotting _separate_ word sets, see the section [Investigate frequencies of two words over time for a single newspaper](#Two-words-in-one-newspaper), below.\n",
    "\n",
    "Finally, be sure the value of `language` corresponds to the language of the newspaper you are looking at (see the table at the top of the page for language information. Note the value should be all lower case; i.e. use 'spanish' _not_ 'Spanish'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for one set of words, one newspaper\n",
    "\n",
    "# Include 'flu' in words to look for\n",
    "title = 'bisbee-daily-review'     # Make sure this matches \"directory\" in table above\n",
    "year_list = ['1917', '1918']      # Each item is separated by a comma\n",
    "my_words = ['flu', 'influenza']   # Each item is separated by a comma\n",
    "language = 'english'              # Can take values 'english', 'spanish' (all lowercase)\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table['Frequency'] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    my_freqs = word_freqs.filter(my_words)\n",
    "    \n",
    "    # Get the total frequency for words of interest\n",
    "    total_my_freq = my_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table['Date'] == issue_date, 'Frequency'] = total_my_freq\n",
    "    \n",
    "# Analyses are all done, plot the figure\n",
    "my_figure = px.line(results_table, x = 'Date', y = 'Frequency')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single word in two newspapers\n",
    "\n",
    "The code below is designed to analyze one set of words for a pair of individual newspaper titles. As written, the code block will look at the frequency of words related to Germany in _El Tucsonense_ and _The Bisbee Daily Review_ during the years 1917-1919. Note because these papers are different languages, we need to provide appropriate word sets for each of the papers.\n",
    "\n",
    "Update the corresponding values for titles, words, and languages for the words and titles of interest. Note the longer the time stretch you are looking at, the longer the analysis may take. When the analysis has fininshed, the asterisk in the square brackets to the lower left will be replaced with a number (i.e. `In [*]` becomes something like `In [6]`) and the plot will be printed below the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for one set of words, two newspapers\n",
    "\n",
    "# Change these to directories of the titles of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title_1 = 'el-tucsonense'\n",
    "title_2 = 'bisbee-daily-review' \n",
    "\n",
    "# The \"human readable\" names of the newspaper titles that will show up on the \n",
    "# plot\n",
    "title_1_name = 'El Tucsonense'\n",
    "title_2_name = 'Bisbee Daily Review'\n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1917', '1918', '1919']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "words_1 = ['alemania', 'alemana', 'alemán'] # germany, german (f.), german (m.)\n",
    "words_2 = ['germany', 'german']\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language_1 = 'spanish'\n",
    "language_2 = 'english'\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Create dictionary with information about each title, for easier\n",
    "# iteration\n",
    "title_data = {}\n",
    "title_data[title_1] = {\n",
    "    'directory' : title_1,\n",
    "    'name' : title_1_name,\n",
    "    'words' : words_1,\n",
    "    'language' : language_1,\n",
    "    'volume_path' : datapath + title_1 + '/volumes/'\n",
    "}\n",
    "title_data[title_1]['volumes'] = os.listdir(title_data[title_1]['volume_path'])\n",
    "title_data[title_2] = {\n",
    "    'directory' : title_2,\n",
    "    'name' : title_2_name,\n",
    "    'words' : words_2,\n",
    "    'language' : language_2,\n",
    "    'volume_path' : datapath + title_2 + '/volumes/'\n",
    "}\n",
    "title_data[title_2]['volumes'] = os.listdir(title_data[title_2]['volume_path'])\n",
    "\n",
    "# Find out all the dates of papers we are looking at\n",
    "dates = []\n",
    "for one_file in (title_data[title_1]['volumes'] + title_data[title_2]['volumes']):\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    # Only add unique values to avoid duplication\n",
    "    if one_date not in dates:\n",
    "        dates.append(one_date)\n",
    "dates.sort()\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to None\n",
    "results_table[title_1_name] = None\n",
    "results_table[title_2_name] = None\n",
    "\n",
    "# Cycle over each title\n",
    "for title in [title_1, title_2]:\n",
    "    title_directory = title_data[title]['directory'] # string\n",
    "    title_name = title_data[title]['name']           # string\n",
    "    words = title_data[title]['words']               # list\n",
    "    language = title_data[title]['language']         # string\n",
    "    volume_path = title_data[title]['volume_path']   # string\n",
    "    volumes = title_data[title]['volumes']           # list\n",
    "\n",
    "    # List of volumes\n",
    "#     volume_path = title_data[title]['volume_path']   # string\n",
    "#     volumes = os.listdir(volume_path)\n",
    "    \n",
    "    # Use date pattern from above to restrict to dates of interest\n",
    "    volumes = list(filter(date_pattern.match, volumes))\n",
    "\n",
    "    # Sort them for easier bookkeeping\n",
    "    volumes.sort()\n",
    "    \n",
    "    # Cycle over all the issues of the current title\n",
    "    printed = False\n",
    "    for issue in volumes:\n",
    "        issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "        \n",
    "        # Clean the text (remove stop words, punctuation, etc.)\n",
    "        issue_text = issue_text.clean_list\n",
    "        \n",
    "        # Create a table with all words from the issue\n",
    "        word_table = pandas.Series(issue_text)\n",
    "        \n",
    "        # Calculate relative frequencies of all words in the issue\n",
    "        word_freqs = word_table.value_counts(normalize = True)\n",
    "        \n",
    "        # Pull out only values that match words of interest\n",
    "        words_freqs = word_freqs.filter(words)\n",
    "\n",
    "        # Get the total frequency for words of interest\n",
    "        total_word_freq = words_freqs.sum()\n",
    "\n",
    "        # Format the date from the name of the file so we know where to put\n",
    "        # the data in our table\n",
    "        issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "\n",
    "        # Add the date & relative frequency to our data table\n",
    "        results_table.loc[results_table['Date'] == issue_date, title_name] = total_word_freq\n",
    "\n",
    "# Analyses are all done, but we need to transform data to \"long\" format\n",
    "results_melt = results_table.melt(id_vars = 'Date', value_vars = [title_1_name, title_2_name])\n",
    "\n",
    "# By default, two columns created are called \"value\" and \"variable\", we want \n",
    "# to rename them\n",
    "results_melt.rename(columns = {'value':'Frequency', 'variable':'Title'}, inplace = True)\n",
    "\n",
    "# Before plotting, remove rows with missing values\n",
    "results_clean = results_melt.dropna()\n",
    "\n",
    "# plot the figure\n",
    "my_figure = px.line(results_clean, x = 'Date' , y = 'Frequency' , color = 'Title')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two words in one newspaper\n",
    "\n",
    "The code below is designed to analyze two sets of words for an individual newspaper title. As written, the code block will look at the frequency of words related to Germany and those related to Japan in _El Tucsonense_ during the years 1917-1919.\n",
    "\n",
    "Update the corresponding values for title, words, and language for the words and title of interest. Note the longer the time stretch you are looking at, the longer the analysis may take. When the analysis has fininshed, the asterisk in the square brackets to the lower left will be replaced with a number (i.e. `In [*]` becomes something like `In [6]`) and the plot will be printed below the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to directory of the title of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title = 'el-tucsonense' \n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1917', '1918', '1919']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "words_1 = ['alemania', 'alemana', 'alemán'] # germany, german (f.), german (m.)\n",
    "words_1_name = 'Germany'\n",
    "words_2 = ['japona', 'japón']\n",
    "words_2_name = 'Japan'\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language = 'spanish'\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table[words_1_name] = 0.0\n",
    "results_table[words_2_name] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    words_1_freqs = word_freqs.filter(words_1)\n",
    "    words_2_freqs = word_freqs.filter(words_2)\n",
    "\n",
    "    # Get the total frequency for words of interest\n",
    "    total_words_1 = words_1_freqs.sum()\n",
    "    total_words_2 = words_2_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + \"-\" + str(issue[4:6]) + \"-\" + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table['Date'] == issue_date, words_1_name] = total_words_1\n",
    "    results_table.loc[results_table['Date'] == issue_date, words_2_name] = total_words_2\n",
    "    \n",
    "# Analyses are all done, but we need to transform data to \"long\" format\n",
    "results_melt = results_table.melt(id_vars = 'Date', value_vars = [words_1_name, words_2_name])\n",
    "\n",
    "# By default, two columns created are called \"value\" and \"variable\", we want \n",
    "# to rename them\n",
    "results_melt.rename(columns = {'value':'Frequency', 'variable':'Words'}, inplace = True)\n",
    "\n",
    "# plot the figure\n",
    "my_figure = px.line(results_melt, x = 'Date' , y = 'Frequency' , color = 'Words')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by.svg)](https://creativecommons.org/licenses/by/4.0/legalcode)\n",
    "\n",
    "This lesson is licensed under a [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode) 2020 to Jeffrey C. Oliver. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
