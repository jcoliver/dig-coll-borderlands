{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Jupyter Notebooks\n",
    "\n",
    "This lesson will introduce the Jupyter Notebook interface. We will use the interface to run and write, yes, write, some Python code for text data analysis.\n",
    "\n",
    "By the end of this lesson, learners should be able to:\n",
    "\n",
    "1. Explain the difference between markdown and code blocks in Jupyter Notebooks\n",
    "2. Execute pre-written Python code to analyze newspaper text\n",
    "3. Modify Python code to change the settings of the analysis\n",
    "\n",
    "And just an aside, all the code for this fun stuff is availble on GitHub at [https://github.com/jcoliver/dig-coll-borderlands](https://github.com/jcoliver/dig-coll-borderlands).\n",
    "\n",
    "If you are not familiar with text data mining, take a look at this nice [StoryMap](https://storymaps.arcgis.com/stories/cd7e273c42cd4ab6b6ce3fa89c13132c) that introduces the idea of text data mining and what we can do with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this Jupyter Notebook thing?\n",
    "\n",
    "Jupyter Notebooks are effectively made up of \"cells\". We can start by thinking of each cell being equivalent to a paragraph on a page. There is an order in which paragraphs and cells appear, and that order matters. In Jupyter Notebooks, the cells come in two flavors and a single notebook (like the one we are working in now) with have both types of cells. \n",
    "\n",
    "+ The first is called \"markdown\", which is text, like you are reading now. We can use some syntax in the text to format the cells in particular ways. For example, we can create italic text by using the underscore symbol (\"\\_\") at the beginning and ending of the text we want to italicize. So when we write \"\\_italic\\_\" in a markdown block, it will show up as _italic_.\n",
    "+ The second kind of cell is a \"code\" cell, that contains computer code in a language like Python or R. This is where the fun comes in.\n",
    "\n",
    "So let's try this out. Click your cursor in the box below on the word \"Data\" and run the cell. You can run the cell by holding down the Control (Ctrl) key and press Enter. You can also click the button labeled \"Run\" at the top of the screen, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Collections as Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we are going to do today is work with some text files on some text data mining questions.\n",
    "\n",
    "### Looking at one file\n",
    "\n",
    "We will start with a single text file.\n",
    "\n",
    "The code block below sets up the name of the file we want to use. There are a couple of important pieces we need to provide:\n",
    "\n",
    "1. The title of the paper, here in a machine-readable form\n",
    "2. The date of the paper, with four digit year, two digit month, and two digit day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'border-vidette'\n",
    "year = '1919'\n",
    "month = '01'\n",
    "day = '04'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which date and which title we are interested in, we need to tell Python where the text files are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to indicate where the data are stored (i.e. which folder \n",
    "# are they in)\n",
    "datapath = 'data/sample/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define another variable, `filename` that contains all the information we need to read the file. That is, `filename` includes the folder location and the filename of the file we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stitch all those pieces of information together, along with the folder \n",
    "# information about where data for an entire day's paper is located\n",
    "filename = datapath + title + '/volumes/' + year + month + day + '.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run that code block, nothing will visibly happen. We haven't asked Python to print anything, and there were no errors (yay!). But we might want to check our work to make sure the file name was specified correctly. So we can use our `print` command again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this time we did not enter a phrase enclosed with quotation marks, but instead provided the word `filename`. But it didn't print \"filename\". Rather it printed the value stored in the _variable_ called `filename`. If you can think back to high school algebra, this is a similar sort of concept - we use a variable, in this case `filename` to store information, much like we would use the variable \"x\" in a mathematical equation.\n",
    "\n",
    "At this point, we are ready to read the file and do some work with it. Before we do so, we will need to tell Python about some additional programs to use. By default, Python does not come with text data mining tools, so those are installed separately and we make them available for use using the `import` command. Run the code block below to load those packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional packages\n",
    "# for data tables\n",
    "import pandas\n",
    "\n",
    "# for file navigation\n",
    "import os\n",
    "\n",
    "# for pattern matching in filenames\n",
    "import re\n",
    "\n",
    "# for text data mining\n",
    "import nltk\n",
    "\n",
    "# for stopword corpora for a variety of languages\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for splitting data into individual words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# for automated text cleaning\n",
    "import digcol as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to download the stopwords. There are _a lot_ of recognized stopwords (i.e. \"y\", \"a\", \"el\", \"la\", \"del\", \"que\", etc.), so we don't want to enter them by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords for several languages\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to read in the data and start looking around. The code block below will read in all the text from the day's paper and clean it up. By \"clean it up\", the `CleanText` does the following:\n",
    "\n",
    "1. Removes stop words (here we use English stop words)\n",
    "2. Removes words that are one character long\n",
    "3. Removes punctuation\n",
    "4. \"Tokenizes\" the data. In this case, that means is breaks the text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = dc.CleanText(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, nothing visibly happened, so we can check our work by looking at the first 20 words. Run the code block below (remember click the box and press Ctrl+Enter or Cmd+Enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdata.clean_list[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this list of words to calculate relative frequency of each word. Relative frequencies in this case are in regards to the length of the issue. We count the number of times a word occurs, and divide that by the total number of words in the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with all the words\n",
    "word_table = pandas.Series(newdata.clean_list)\n",
    "\n",
    "# Calculate relative frequency of each word\n",
    "word_freqs = word_table.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our work, we look at the first 10 rows of the `word_freqs` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_freqs.head(n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should come as no big surprise that \"Arizona\" and \"Nogales\" are the most frequent words, given that the paper was printed in Nogales, Arizona.\n",
    "\n",
    "## Beyond counting\n",
    "\n",
    "Now we can broaden our focus to look at trends over time. We are going to look a multiple years of papers to track how the frequency of influenza coverages changes over time. We will stick with _The Border Vidette_ but instead of looking at a single issue, we will look at all the issues 1917-1919.\n",
    "\n",
    "Here's where it gets fun. We could try to do this file-by-file, but that would be extremely tedious. So we are going to give Python a little bit of information and let the computer look at every single file. But first we need to tell Python _which_ files to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern that will match the dates of interest. In this case, \n",
    "# papers from 1917, 1918, and 1919\n",
    "date_pattern = re.compile(r'(1917|1918|1919)([0-9]{4})*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what the hell does that even mean? What we have in the code block above is something called \"regular expressions\". Regular expressions is a very powerful pattern matching tool with a very terrible name. What we are saying above is that we want any files that:\n",
    "\n",
    "+ start with \"1917\", \"1918\", or \"1919\",\n",
    "+ followed by four digits, i.e. the two-digit month and two-digit day, so May 1 is represented as \"0501\"\n",
    "    + `[0-9]` matches any single digit between 0 and 9\n",
    "    + `{4}` means that there are four consecutive digits. `[0-9][0-9][0-9][0-9]` is equivalent to `[0-9]{4}`, we just don't have to write as much\n",
    "+ and end with anything (the asterisk \"\\*\" is a wild card, matching and letters, numbers, or symbols)\n",
    "+ that extra \"r\" right before the quotation marks helps python treat the expression correctly (it's OK, I don't quite understand why it is necessary either, but smarter people than me said it is a good idea).\n",
    "\n",
    "So be sure you run the code block above (Ctrl-Enter or Cmd-Enter) before moving on. You will know that the code block has been run when you see a number show up in between the square brackets to the left of the code block (`In [ ]:`).\n",
    "\n",
    "We are now ready to start reading in the files. We need to start by listing _all_ the _Border Vidette_ issues, then filtering only those that are in the date range of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the Border Vidette files and store in bv_volumes variable\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "bv_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "bv_volumes = list(filter(date_pattern.match, bv_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "bv_volumes.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another opportunity for a reality check, so we ask Python to print out the first five files that we will ask Python to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bv_volumes[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know which files to look at, so we can instruct Python to do so. We are ultimately going to want to create a table that has two columns of data:\n",
    "\n",
    "1. The date the paper was published. Thankfully, this is stored in the filename: the file 19170113.txt is the paper that was published on January 13, 1917.\n",
    "2. The relative frequency of the words we are interested in for that date's paper\n",
    "\n",
    "We will start by creating a table that will hold that information. We need to extract dates for each paper. While the filenames have that information, we need to convert it to an actual date, in the form of YYYY-MM-DD, so the date for 19170113.txt is 1917-01-13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in bv_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait. No. What?\n",
    "\n",
    "So there's a bit going on there up above.\n",
    "\n",
    "+ `dates = []` creates an empty list of dates. There is nothing in there to start with.\n",
    "+ `for one_file in bv_volumes:` says we are going to cycle through all of the files that are listed in the `bv_volumes` variable; each cycle, the value of `one_file` changes to the next value. If you look at the output we created above when running `print(bv_volumes[0:5])`, the first time through the cycle, `one_file` will have the value '19170113.txt'. The second time, `one_file` will have the value '19170120.txt'.\n",
    "+ `one_date = str(one_file[0:4]) + str(one_file[4:6]) + str(one_file[6:8])` is creating a - no, we just need to run some code to explain this one.\n",
    "\n",
    "Let us do a little test, seeing what this code does on an example of `one_file`. We start by pulling out the very first value in `bv_volumes`, as if it was the first cycle through the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_file = bv_volumes[0]\n",
    "print(one_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Looking good. What we are doing with the `one_date` line is pulling out parts of that filename using indexing. An index is basically an address for each letter. For the first That is, we pull out the 0<sup>th</sup> through 3<sup>rd</sup> part of the file name via `one_file[0:4]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first four characters\n",
    "print(one_file[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at characters 5 and 6\n",
    "print(one_file[4:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the entireity of the filename, these are the indexes of each character:\n",
    "\n",
    "| Index:      | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |\n",
    "|-------------|---|---|---|---|---|---|---|---|---|---|----|----|\n",
    "| Characters: | 1 | 9 | 1 | 7 | 0 | 1 | 1 | 3 | . | t | x  | t  |\n",
    "\n",
    "If we run the piece of code that stitches all the pieces together,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see the date formatted like we want (i.e. YYYY-MM-DD). \n",
    "\n",
    "Looking back to the cycle:\n",
    "\n",
    "```\n",
    "for one_file in bv_volumes:\n",
    "    one_date = str(one_file[0:4]) + \"-\" + str(one_file[4:6]) + \"-\" + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "```\n",
    "\n",
    "the last line (`dates.append(one_date)`) will add that one date to our list of dates. Now that we have all those dates, we can set up our table (finally!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add those dates to a data frame\n",
    "flu_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "flu_table['Frequency'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a reality check, let's look at the first six rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_table.head(n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Python to cycle over every file, calculate the relative frequency of flu and influenza, and store the result in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_words = ['flu', 'influenza']\n",
    "\n",
    "for issue in bv_volumes:\n",
    "    issue_text = dc.CleanText(volume_path + issue)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values for flu or influenza\n",
    "    flu_freqs = word_freqs.filter(flu_words)\n",
    "    \n",
    "    # Get the total frequency for flu and influenza\n",
    "    total_flu_freq = flu_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    flu_table.loc[flu_table['Date'] == issue_date, 'Frequency'] = total_flu_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look again at the first six rows of our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_table.head(n = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...they are all still zeros. But maybe that isn't surprising, since the influenza pandemic did not really get going until late 1918, and we are just looking at the early 1917 issues here. When doing this sort of quality assurance, we can pick a paper that we _know_ will have at least some occurrences of influenza. The November 16 issue from 1918 had at least some mention of influenza, so we can look at the corresponding row for that date via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a 1918 date where we know there should be non-zero values\n",
    "flu_table.loc[flu_table['Date'] == '1918-11-16', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We have our data ready to go. All we need to do now is graph it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more package is needed for plotting\n",
    "import plotly.express as px\n",
    "flu_figure = px.line(flu_table, x = 'Date', y = 'Frequency')\n",
    "flu_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph should show a peak in relative frequency of flu/influenza over the winter of 1918-1919.\n",
    "\n",
    "## Your turn\n",
    "\n",
    "The code block below includes all the code necessary to make a graph like the one above, but you get to determine what it shows. You'll need to provide:\n",
    "\n",
    "1. The title of the newspaper you want to analyze\n",
    "2. The range of years to include (note, consult the table below for available date ranges for each paper)\n",
    "3. The words you are interested in including\n",
    "\n",
    "Run the code block below to display a table with relevant information regarding available titles and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to display table with newspaper information\n",
    "titles = pandas.read_csv('data/sample/sample-titles.csv')\n",
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the first four variables in the code block below then run the code block. Your graph should appear below the block once it has finished running. You'll know it finished when the asterisk in the square brackets is replaced by a number (e.g. `In [*]` -> `In [31]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to directory of the title of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title = 'el-tucsonense' \n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1917', '1918', '1919']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "my_words = ['alemania', 'alemana', 'alemán'] # germany, german (f.), german (m.)\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language = 'spanish'\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table['Frequency'] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    my_freqs = word_freqs.filter(my_words)\n",
    "    \n",
    "    # Get the total frequency for words of interest\n",
    "    total_my_freq = my_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table['Date'] == issue_date, 'Frequency'] = total_my_freq\n",
    "    \n",
    "# Analyses are all done, plot the figure\n",
    "my_figure = px.line(results_table, x = 'Date', y = 'Frequency')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than one line?\n",
    "\n",
    "If you want to plot more than one line on a plot, you can use the code below. The code below is set up for plotting two lines for one title of one language, but it could be extended to multiple lines, titles, and languages (but probably not today)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to directory of the title of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title = 'el-tucsonense' \n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1917', '1918', '1919']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "words_1 = ['alemania', 'alemana', 'alemán'] # germany, german (f.), german (m.)\n",
    "words_1_name = 'German'\n",
    "words_2 = ['japona', 'japón']\n",
    "words_2_name = 'Japan'\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language = 'spanish'\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table[words_1_name] = 0.0\n",
    "results_table[words_2_name] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    words_1_freqs = word_freqs.filter(words_1)\n",
    "    words_2_freqs = word_freqs.filter(words_2)\n",
    "\n",
    "    # Get the total frequency for words of interest\n",
    "    total_words_1 = words_1_freqs.sum()\n",
    "    total_words_2 = words_2_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table['Date'] == issue_date, words_1_name] = total_words_1\n",
    "    results_table.loc[results_table['Date'] == issue_date, words_2_name] = total_words_2\n",
    "    \n",
    "# Analyses are all done, but we need to transform data to \"long\" format\n",
    "results_melt = results_table.melt(id_vars = 'Date', value_vars = [words_1_name, words_2_name])\n",
    "\n",
    "# By default, two columns created are called \"value\" and \"variable\", we want \n",
    "# to rename them\n",
    "results_melt.rename(columns = {'value':'Frequency', 'variable':'Words'}, inplace = True)\n",
    "\n",
    "# plot the figure\n",
    "my_figure = px.line(results_melt, x = 'Date' , y = 'Frequency' , color = 'Words')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative analysis\n",
    "\n",
    "We can also make comparisons between different titles. Here we are going to compare the _Bisbee Daily Review_ and the _Border Vidette_ to see if there is a difference in the coverage of the [mine strike of 1917](https://en.wikipedia.org/wiki/Bisbee_Deportation#Strike).\n",
    "\n",
    "We start as we did before to filter papers by dates of interest. We are going to focus only on those issues published June through October of 1917."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern that will match papers June - October, 1917\n",
    "date_pattern = re.compile(r'1917(06|07|08|09|10)([0-9]{2})*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More regular expressions! We are only looking at papers published in 1917, in the months June (06) through October (10). Similar to the regular expression we saw earlier, this allows us to match those files that:\n",
    "\n",
    "+ start with \"1917\",\n",
    "+ followed by either \"06\", \"07\", \"08\", \"09\", or \"10\", (the month specification)\n",
    "+ followed by two digits (the day specification)\n",
    "    + `[0-9]` matches any single digit between 0 and 9\n",
    "    + `{2}` means that there are two consecutive digits, so in this case we are using `[0-9]{2}` as a shortcut for `[0-9][0-9]`\n",
    "+ and end with anything (the asterisk \"\\*\" is a wild card, matching and letters, numbers, or symbols)\n",
    "\n",
    "We match only those files, for each title, that were published during the time period of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the data from each title separate, we will use a convention where \n",
    "# the variables are named such that the prefix of the variable name \n",
    "# indicates the title from which the information is associated with:\n",
    "# bv = Border Vidette\n",
    "# bdr = Bisbee Daily Review\n",
    "\n",
    "# List all the Border Vidette files\n",
    "bv_volumes = os.listdir('data/sample/border-vidette/volumes')\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "bv_volumes = list(filter(date_pattern.match, bv_volumes))\n",
    "\n",
    "# Do a little reality check to make sure we only see files in \n",
    "# desired date range.\n",
    "print('Border Vidette: ' + str(len(bv_volumes)) + ' issues')\n",
    "print(bv_volumes[0:5])\n",
    "\n",
    "# Download and filter files for Bisbee Daily Review (like above)\n",
    "bdr_volumes = os.listdir('data/sample/bisbee-daily-review/volumes')\n",
    "bdr_volumes = list(filter(date_pattern.match, bdr_volumes))\n",
    "\n",
    "# Another reality check, reporting total number of issues and the \n",
    "# first five filenames\n",
    "print('Bisbee Daily Review: ' + str(len(bdr_volumes)) + ' issues')\n",
    "print(bdr_volumes[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did above with the search for influenza terms, we will look for words related to the strike in Bisbee, and calculate the relative frequency for each issue of each title. We start by defining the terms to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the words of interest\n",
    "strike_words = ['strike', 'strikes', 'striker', 'strikers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We start with looking at issues of _Border Vidette_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all Border Vidette volumes that matched our date criteria, calculate \n",
    "# the relative frequency of 'strike' and related words\n",
    "\n",
    "# This variable will hold relative frequency for each day's paper\n",
    "bv_strike_freq = []\n",
    "\n",
    "# The location where text for each issue is stored\n",
    "bv_file_locations = datapath + 'border-vidette/volumes/'\n",
    "\n",
    "# Loop over each issue, calculating relative frequency\n",
    "for one_issue in bv_volumes:\n",
    "    # Read in the cleaned text (stopwords and punctuation removed)\n",
    "    issue_text = dc.CleanText(filename = bv_file_locations + one_issue)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with all the words in the issue\n",
    "    word_table = pandas.Series(issue_text)\n",
    "    \n",
    "    # Calculate relative frequency of each word in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values for strike related words\n",
    "    strike_freqs = word_freqs.filter(strike_words)\n",
    "    \n",
    "    # Add those frequencies to our list of values for Border Vidette\n",
    "    bv_strike_freq.append(strike_freqs.sum())\n",
    "\n",
    "# Do a reality check to look at first five values\n",
    "print(bv_strike_freq[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the process of calculating relative frequencies for _Bisbee Daily Review_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all Bisbee Daily Review volumes that matched our date criteria, \n",
    "# calculate the relative frequency of 'strike' and related words\n",
    "\n",
    "# This variable will hold relative frequency for each day's paper\n",
    "bdr_strike_freq = []\n",
    "\n",
    "# The location where text for each issue is stored\n",
    "bdr_file_locations = datapath + 'bisbee-daily-review/volumes/'\n",
    "\n",
    "# Loop over each issue, calculating relative frequency\n",
    "for one_issue in bdr_volumes:\n",
    "    # Read in the cleaned text (stopwords and punctuation removed)\n",
    "    issue_text = dc.CleanText(filename = bdr_file_locations + one_issue)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with all the words in the issue\n",
    "    word_table = pandas.Series(issue_text)\n",
    "    \n",
    "    # Calculate relative frequency of each word in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values for strike related words\n",
    "    strike_freqs = word_freqs.filter(strike_words)\n",
    "    \n",
    "    # Add those frequencies to our list of values for Border Vidette\n",
    "    bdr_strike_freq.append(strike_freqs.sum())\n",
    "\n",
    "# Do a reality check to look at first five values\n",
    "print(bdr_strike_freq[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the relative frequencies for each of the titles, we can calculate some summary statistics, including the average relative frequency in each issue for each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mean function from the statistics package\n",
    "from statistics import mean\n",
    "\n",
    "# Calculate average relative frequency of Border Vidette issues\n",
    "bv_mean = mean(bv_strike_freq)\n",
    "\n",
    "# Print the value, using format instead of str to avoid scientific notation\n",
    "print(format(bv_mean, 'f') + ' Border Vidette')\n",
    "\n",
    "# Calculate average relative frequency of Bisbee Daily Review issues and print \n",
    "# to screen\n",
    "bdr_mean = mean(bdr_strike_freq)\n",
    "print(format(bdr_mean, 'f') + ' Bisbee Daily Review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these means, we see that the relative frequency of strike-related words was higher in issues of the _Bisbee Daily Review_ than in issues of the _Border Vidette_. In fact, it looks like the relative frequency of strike words in the _Bisbee Daily Review_ was ten-fold higher than in the _Border Vidette_.\n",
    "\n",
    "Finally, we need to run a statistical test to see if those means are significantly difference. For our purposes, we can use a two-sample _t_-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scipy package has a stats function that allows us to run a t-test\n",
    "from scipy import stats\n",
    "\n",
    "# Run the test, assuming unequal variances\n",
    "compare_strike = stats.ttest_ind(bv_strike_freq, bdr_strike_freq, equal_var = False)\n",
    "\n",
    "# Extract values of interest, Student's t and the p-value\n",
    "t_value = compare_strike[0]\n",
    "p_value = compare_strike[1]\n",
    "\n",
    "# Print test statistics\n",
    "print('t = ' + format(t_value, '.3f')) # normal formatting\n",
    "print('p = ' + format(p_value, '.3e')) # scientific notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can conclude the relative word frequency of strike-related words _was_ significantly higher in the _Bisbee Daily Review_ than in the _Border Vidette_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew. We're done.\n",
    "\n",
    "If you want more practice or want to do some analyses with a larger data set, head over to the lesson at [https://mybinder.org/v2/gh/jcoliver/dig-coll-borderlands/master?filepath=Text-Mining-Template.ipynb](https://mybinder.org/v2/gh/jcoliver/dig-coll-borderlands/master?filepath=Text-Mining-Template.ipynb).\n",
    "\n",
    "If you want even _more_ resources, check these out:\n",
    "\n",
    "+ Examples of text data mining:\n",
    "    + Benjamin Schmidt and Mitch Fraas. [The Language of the State of the Union](https://www.theatlantic.com/politics/archive/2015/01/the-language-of-the-state-of-the-union/384575/). _The Atlantic_ (January 18, 2015). \n",
    "    + Lincoln Mullen. [America’s Public Bible: Biblical Quotations in U.S. Newspapers](https://americaspublicbible.org/).(2016).\n",
    "+ Text data mining in Python:\n",
    "    + Quinn Dombrowski, Tassie Gniady, and David Kloster, \"Introduction to Jupyter Notebooks,\" The Programming Historian 8 (2019), [https://doi.org/10.46430/phen0087](https://doi.org/10.46430/phen0087).\n",
    "    + William J. Turkel and Adam Crymble, \"Counting Word Frequencies with Python,\" The Programming Historian 1 (2012), [https://doi.org/10.46430/phen0003](https://doi.org/10.46430/phen0003).\n",
    "\n",
    "If you have any questions or comments on this lesson, look at the [project's GitHub page](https://github.com/jcoliver/dig-coll-borderlands) and open a new issue if you don't find an answer there.\n",
    "\n",
    "[![](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by.svg)](https://creativecommons.org/licenses/by/4.0/legalcode)\n",
    "\n",
    "This lesson is licensed under a [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode) 2020 to Jeffrey C. Oliver. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
