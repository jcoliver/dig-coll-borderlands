{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla de minería de texto\n",
    "Este cuaderno incluye código Python para descargar texto escaneado de periódicos fronterizos y realizar análisis de texto de frecuencia de palabras en los periódicos.\n",
    "\n",
    "Para comenzar, siga las instrucciones en la sección [Configuración](#Configuración), a continuación. Una vez que sepa qué datos le gustaría usar, hay varias opciones enumeradas a continuación para diferentes análisis de texto.\n",
    "\n",
    "El trabajo es parte de dos proyectos:\n",
    "+ _Using Newspapers as Data for Collaborative Pedagogy: A Multidisciplinary Interrogation of the Borderlands in Undergraduate Classrooms_ (_El Uso de Periódicos como Datos para la Pedagogía Colaborativa: Una Interrogación Multidisciplinaria de las Fronteras en las Aulas de Pregrado_), financiado en parte por la Fundación Mellon a través de la programa [Collections as Data](https://collectionsasdata.github.io/part2whole/). Más información sobre el proyecto está disponible en\n",
    "[https://libguides.library.arizona.edu/newspapers-as-data](https://libguides.library.arizona.edu/newspapers-as-data).\n",
    "+ _Reporting on Race and Ethnicity in the Borderlands (1882-1924): A Data-Driven Digital Storytelling Hub_ (_Reportaje sobre Raza y Etnicidad en las Tierras Fronterizas (1882-1924): Un Centro de Narración Digital Basado en Datos_), financiado por la Fundación Mellon a través de el [Digital Borderlands](http://borderlands.digitalscholarship.library.arizona.edu/) programa.\n",
    "\n",
    "Si no está familiarizado con la minería de datos de texto, eche un vistazo a este agradable [StoryMap](https://arcg.is/1j84jz) que presenta la idea de la minería de datos de texto y lo que podemos hacer con ella.\n",
    "\n",
    "Este cuaderno y las lecciones adicionales de minería de texto están disponibles en [https://github.com/jcoliver/dig-coll-borderlands](https://github.com/jcoliver/dig-coll-borderlands)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración\n",
    "La primera decisión que debe tomar es si desea utilizar un conjunto de datos de muestra pequeño o un conjunto de datos más grande. La última opción requiere que se descarguen los archivos y puede tardar unos minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si _no_ desea usar el conjunto más grande de texto escaneado, puede usar los datos que se distribuyen con este cuaderno. Ejecutar el bloque de código a continuación le mostrará los datos que están disponibles si no desea descargar el conjunto de datos más grande (no necesita realizar ningún paso adicional para usar los datos a continuación, vienen con este Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to display table with newspaper information\n",
    "import pandas\n",
    "titles = pandas.read_csv('data/sample/sample-titles.csv')\n",
    "datapath = 'data/sample/'\n",
    "display(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desea utilizar el conjunto completo de periódicos fronterizos escaneados, primero deberá descargar los archivos del depósito de datos de la Universidad de Arizona. Ejecutar el bloque de código a continuación hará esto por usted (si solo quiere probar cosas con un conjunto de datos más pequeño, no ejecute este bloque y simplemente avance). Tenga en cuenta que los datos están contenidos en un archivo de alrededor de 1,5 GB e incluyen cientos de miles de archivos. Tanto los pasos de descarga como los de extracción de archivos pueden demorar un poco (¿5 minutos? ¿10?), por lo que ahora podría ser un buen momento para volver a llenar su bebida. Cuando se complete el proceso de descarga y extracción, se imprimirá una tabla que muestra los datos disponibles debajo del bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries necessary for download & extraction\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "# Location of the file on the UA Data Repository\n",
    "url = 'https://arizona.figshare.com/ndownloader/files/31104157'\n",
    "\n",
    "# Download the file & write it to disk\n",
    "zip_filename = 'fulldata.zip'\n",
    "download = urlretrieve(url, zip_filename)\n",
    "\n",
    "# Set the destination for the data files\n",
    "destination = 'data/complete/'\n",
    "\n",
    "# Make sure the destination directory exists\n",
    "if(not(os.path.isdir(destination))):\n",
    "    os.makedirs(destination)\n",
    "\n",
    "# Extract files to destination directory\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zipdata:\n",
    "    zipdata.extractall(destination)\n",
    "    \n",
    "# No need for that zipfile, so we can remove it\n",
    "os.remove(zip_filename)\n",
    "\n",
    "# Finally, display the available titles for this full data set\n",
    "full_titles = pandas.read_csv('data/complete/complete-titles.csv')\n",
    "datapath = 'data/complete/'\n",
    "display(full_titles.sort_values(by=['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de frecuencia de palabras\n",
    "Para los periódicos de su elección, hay una variedad de análisis que se pueden realizar con el código a continuación.\n",
    "+ [Investigar la frecuencia de palabras a lo largo del tiempo para una sola palabra (o conjunto de palabras) en un solo periódico](#Una-sola-palabra-en-un-periódico)\n",
    "+ [Investigar la frecuencia de palabras a lo largo del tiempo en un par de periódicos](#Una-sola-palabra-en-dos-periódicos)\n",
    "+ [Investigar frecuencias de dos palabras a lo largo del tiempo para un solo periódico](#Dos-palabras-en-un-periódico)\n",
    "\n",
    "En todos los análisis, el siguiente código tiene valores de ejemplo para periódicos, palabras y fechas. Puede cambiarlos según sea necesario para su pregunta específica.\n",
    "\n",
    "Sin embargo, antes de comenzar, asegúrese de ejecutar el bloque de código inmediatamente debajo, que se carga en todas las bibliotecas necesarias para los análisis de datos de texto posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to change anything, just run this block of code to load necessary libraries\n",
    "\n",
    "# for data tables\n",
    "import pandas\n",
    "\n",
    "# for file navigation\n",
    "import os\n",
    "\n",
    "# for pattern matching in filenames\n",
    "import re\n",
    "\n",
    "# for text data mining\n",
    "import nltk\n",
    "\n",
    "# for stopword corpora for a variety of languages\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for splitting data into individual words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# for automated text cleaning\n",
    "import digcol as dc\n",
    "\n",
    "# download the stopwords for several languages\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# for drawing the plot\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una sola palabra en un periódico\n",
    "El siguiente código está diseñado para analizar un conjunto de palabras para un título de periódico individual. Tal como está escrito, el bloque de código observará la frecuencia de las palabras relacionadas con la influenza (\"gripe\", \"influenza\") en _The Bisbee Daily Review_ durante los años 1917 y 1918.\n",
    "\n",
    "Puede editar los valores de `título`, `lista_años`, `mis_palabras` e `idioma` para que se ajusten a su análisis de interés. Para el valor de `título`, asegúrese de utilizar el valor de la columna \"directorio\" de la tabla anterior que corresponda al periódico de interés. Por ejemplo, si quisieras mirar _El Tucsonense_, cambia esto:\n",
    "\n",
    "`title = 'revisión-diaria-de-bisbee'`\n",
    "\n",
    "a esto:\n",
    "\n",
    "`title = 'el-tucsonense'`\n",
    "\n",
    "Para `year_list`, enumere todos los años de interés, cada uno entre comillas simples (') y valores separados por una coma. Si solo está interesado en un año, no es necesaria la coma.\n",
    "\n",
    "Las palabras listadas en `mis_palabras` serán efectivamente \"agrupadas\"; es decir, para este ejemplo, la gráfica mostrará la frecuencia de 'gripe' e 'influenza' combinadas. Si está interesado en trazar conjuntos de palabras _separados_, consulte la sección [Investigar frecuencias de dos palabras a lo largo del tiempo para un solo periódico](#Dos-palabras-en-un-periódico), a continuación.\n",
    "\n",
    "Finalmente, asegúrese de que el valor de 'idioma' corresponda al idioma del periódico que está viendo (consulte la tabla en la parte superior de la página para obtener información sobre el idioma. Tenga en cuenta que el valor debe estar en minúsculas; es decir, use 'español' _no_ 'Español'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for one set of words, one newspaper\n",
    "\n",
    "# Include 'flu' in words to look for\n",
    "title = 'bisbee-daily-review'     # Make sure this matches \"directory\" in table above\n",
    "year_list = ['1917', '1918']      # Each item is separated by a comma\n",
    "my_words = ['flu', 'influenza']   # Each item is separated by a comma\n",
    "language = 'english'              # Can take values 'english', 'spanish' (all lowercase)\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table['Frequency'] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    my_freqs = word_freqs.filter(my_words)\n",
    "    \n",
    "    # Get the total frequency for words of interest\n",
    "    total_my_freq = my_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table['Date'] == issue_date, 'Frequency'] = total_my_freq\n",
    "    \n",
    "# Analyses are all done, plot the figure\n",
    "my_figure = px.line(results_table, x = 'Date', y = 'Frequency')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una sola palabra en dos periódicos\n",
    "El siguiente código está diseñado para analizar un conjunto de palabras para un par de títulos de periódicos individuales. Tal como está escrito, el bloque de código observará la frecuencia de palabras relacionadas con Alemania en _El Tucsonense_ y _The Bisbee Daily Review_ durante los años 1917-1919. Tenga en cuenta que debido a que estos documentos son en diferentes idiomas, debemos proporcionar conjuntos de palabras apropiados para cada uno de los documentos.\n",
    "\n",
    "Actualice los valores correspondientes de títulos, palabras e idiomas para las palabras y títulos de interés. Tenga en cuenta que cuanto más largo sea el intervalo de tiempo que está observando, más tiempo puede llevar el análisis. Cuando el análisis haya finalizado, el asterisco entre corchetes en la parte inferior izquierda se reemplazará con un número (es decir, `In [*]` becomes something like `In [6]`) y el gráfico se imprimirá debajo del bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for one set of words, two newspapers\n",
    "\n",
    "# Change these to directories of the titles of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title_1 = 'el-tucsonense'\n",
    "title_2 = 'bisbee-daily-review' \n",
    "\n",
    "# The \"human readable\" names of the newspaper titles that will show up on the \n",
    "# plot\n",
    "title_1_name = 'El Tucsonense'\n",
    "title_2_name = 'Bisbee Daily Review'\n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1917', '1918', '1919']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "words_1 = ['alemania', 'alemana', 'alemán'] \n",
    "words_2 = ['germany', 'german']\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language_1 = 'spanish'\n",
    "language_2 = 'english'\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "# Create dictionary with information about each title, for easier\n",
    "# iteration\n",
    "title_data = {}\n",
    "title_data[title_1] = {\n",
    "    'directory' : title_1,\n",
    "    'name' : title_1_name,\n",
    "    'words' : words_1,\n",
    "    'language' : language_1,\n",
    "    'volume_path' : datapath + title_1 + '/volumes/'\n",
    "}\n",
    "title_data[title_1]['volumes'] = os.listdir(title_data[title_1]['volume_path'])\n",
    "title_data[title_2] = {\n",
    "    'directory' : title_2,\n",
    "    'name' : title_2_name,\n",
    "    'words' : words_2,\n",
    "    'language' : language_2,\n",
    "    'volume_path' : datapath + title_2 + '/volumes/'\n",
    "}\n",
    "title_data[title_2]['volumes'] = os.listdir(title_data[title_2]['volume_path'])\n",
    "\n",
    "# Find out all the dates of papers we are looking at\n",
    "dates = []\n",
    "for one_file in (title_data[title_1]['volumes'] + title_data[title_2]['volumes']):\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    # Only add unique values to avoid duplication\n",
    "    if one_date not in dates:\n",
    "        dates.append(one_date)\n",
    "dates.sort()\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to None\n",
    "results_table[title_1_name] = None\n",
    "results_table[title_2_name] = None\n",
    "\n",
    "# Cycle over each title\n",
    "for title in [title_1, title_2]:\n",
    "    title_directory = title_data[title]['directory'] # string\n",
    "    title_name = title_data[title]['name']           # string\n",
    "    words = title_data[title]['words']               # list\n",
    "    language = title_data[title]['language']         # string\n",
    "    volume_path = title_data[title]['volume_path']   # string\n",
    "    volumes = title_data[title]['volumes']           # list\n",
    "\n",
    "    # List of volumes\n",
    "#     volume_path = title_data[title]['volume_path']   # string\n",
    "#     volumes = os.listdir(volume_path)\n",
    "    \n",
    "    # Use date pattern from above to restrict to dates of interest\n",
    "    volumes = list(filter(date_pattern.match, volumes))\n",
    "\n",
    "    # Sort them for easier bookkeeping\n",
    "    volumes.sort()\n",
    "    \n",
    "    # Cycle over all the issues of the current title\n",
    "    printed = False\n",
    "    for issue in volumes:\n",
    "        issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "        \n",
    "        # Clean the text (remove stop words, punctuation, etc.)\n",
    "        issue_text = issue_text.clean_list\n",
    "        \n",
    "        # Create a table with all words from the issue\n",
    "        word_table = pandas.Series(issue_text)\n",
    "        \n",
    "        # Calculate relative frequencies of all words in the issue\n",
    "        word_freqs = word_table.value_counts(normalize = True)\n",
    "        \n",
    "        # Pull out only values that match words of interest\n",
    "        words_freqs = word_freqs.filter(words)\n",
    "\n",
    "        # Get the total frequency for words of interest\n",
    "        total_word_freq = words_freqs.sum()\n",
    "\n",
    "        # Format the date from the name of the file so we know where to put\n",
    "        # the data in our table\n",
    "        issue_date = str(issue[0:4]) + '-' + str(issue[4:6]) + '-' + str(issue[6:8])\n",
    "\n",
    "        # Add the date & relative frequency to our data table\n",
    "        results_table.loc[results_table['Date'] == issue_date, title_name] = total_word_freq\n",
    "\n",
    "# Analyses are all done, but we need to transform data to \"long\" format\n",
    "results_melt = results_table.melt(id_vars = 'Date', value_vars = [title_1_name, title_2_name])\n",
    "\n",
    "# By default, two columns created are called \"value\" and \"variable\", we want \n",
    "# to rename them\n",
    "results_melt.rename(columns = {'value':'Frequency', 'variable':'Title'}, inplace = True)\n",
    "\n",
    "# Before plotting, remove rows with missing values\n",
    "results_clean = results_melt.dropna()\n",
    "\n",
    "# plot the figure\n",
    "my_figure = px.line(results_clean, x = 'Date' , y = 'Frequency' , color = 'Title')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dos palabras en un periódico\n",
    "El siguiente código está diseñado para analizar dos conjuntos de palabras para un título de periódico individual. Tal como está escrito, el bloque de código observará la frecuencia de palabras relacionadas con Alemania y las relacionadas con Japón en _El Tucsonense_ durante los años 1917-1919.\n",
    "\n",
    "Actualice los valores correspondientes de título, palabras e idioma para las palabras y el título de interés. Tenga en cuenta que cuanto más largo sea el intervalo de tiempo que está observando, más tiempo puede llevar el análisis. Cuando el análisis haya finalizado, el asterisco entre corchetes en la parte inferior izquierda se reemplazará con un número (es decir, `In [*]` se convierte en algo como `In [6]`) y el gráfico se imprimirá debajo del bloque de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to directory of the title of interest, be sure to use lowercase \n",
    "# and no spaces (it may be easiest to copy & paste from the \"directory\" \n",
    "# column in the table above)\n",
    "title = 'el-tucsonense' \n",
    "\n",
    "# List the years of interest, each enclosed in quotation marks (') and separated\n",
    "# by commas\n",
    "year_list = ['1917', '1918', '1919']\n",
    "\n",
    "# What words are you interested in? You can add as many as you like, \n",
    "# just be sure to enclose each in quotation marks (') and separate with a comma\n",
    "# Also, keep them lower case, even if they are proper nouns\n",
    "words_1 = ['alemania', 'alemana', 'alemán']\n",
    "words_1_name = 'Germany'\n",
    "words_2 = ['japona', 'japón']\n",
    "words_2_name = 'Japan'\n",
    "\n",
    "# Specify the language of the title you are looking at (all lowercase)\n",
    "# Possible values: english, spanish, arabic, turkish, etc.\n",
    "language = 'spanish'\n",
    "\n",
    "################################################################################\n",
    "# No need to edit anything below here\n",
    "################################################################################\n",
    "\n",
    "# Creating the pattern of filenames based on years to match\n",
    "years = '|'\n",
    "years = years.join(year_list)\n",
    "pattern = '(' + years + ')([0-9]{4})*'\n",
    "date_pattern = re.compile(pattern)\n",
    "\n",
    "\n",
    "# Location of files with text for a day's paper\n",
    "volume_path = datapath + title + '/volumes/'\n",
    "my_volumes = os.listdir(volume_path)\n",
    "\n",
    "# Use date pattern from above to restrict to dates of interest\n",
    "my_volumes = list(filter(date_pattern.match, my_volumes))\n",
    "\n",
    "# Sort them for easier bookkeeping\n",
    "my_volumes.sort()\n",
    "\n",
    "# Create a table that will hold the relative frequency for each date\n",
    "dates = []\n",
    "for one_file in my_volumes:\n",
    "    one_date = str(one_file[0:4]) + '-' + str(one_file[4:6]) + '-' + str(one_file[6:8])\n",
    "    dates.append(one_date)\n",
    "\n",
    "# Add those dates to a data frame\n",
    "results_table = pandas.DataFrame(dates, columns = ['Date'])\n",
    "\n",
    "# Set all frequencies to zero\n",
    "results_table[words_1_name] = 0.0\n",
    "results_table[words_2_name] = 0.0\n",
    "\n",
    "# Cycle over all issues and do relative frequency calculations\n",
    "for issue in my_volumes:\n",
    "    issue_text = dc.CleanText(filename = volume_path + issue, language = language)\n",
    "    issue_text = issue_text.clean_list\n",
    "    \n",
    "    # Create a table with words\n",
    "    word_table = pandas.Series(issue_text)\n",
    "\n",
    "    # Calculate relative frequencies of all words in the issue\n",
    "    word_freqs = word_table.value_counts(normalize = True)\n",
    "    \n",
    "    # Pull out only values that match words of interest\n",
    "    words_1_freqs = word_freqs.filter(words_1)\n",
    "    words_2_freqs = word_freqs.filter(words_2)\n",
    "\n",
    "    # Get the total frequency for words of interest\n",
    "    total_words_1 = words_1_freqs.sum()\n",
    "    total_words_2 = words_2_freqs.sum()\n",
    "    \n",
    "    # Format the date from the name of the file so we know where to put\n",
    "    # the data in our table\n",
    "    issue_date = str(issue[0:4]) + \"-\" + str(issue[4:6]) + \"-\" + str(issue[6:8])\n",
    "    \n",
    "    # Add the date & relative frequency to our data table\n",
    "    results_table.loc[results_table['Date'] == issue_date, words_1_name] = total_words_1\n",
    "    results_table.loc[results_table['Date'] == issue_date, words_2_name] = total_words_2\n",
    "    \n",
    "# Analyses are all done, but we need to transform data to \"long\" format\n",
    "results_melt = results_table.melt(id_vars = 'Date', value_vars = [words_1_name, words_2_name])\n",
    "\n",
    "# By default, two columns created are called \"value\" and \"variable\", we want \n",
    "# to rename them\n",
    "results_melt.rename(columns = {'value':'Frequency', 'variable':'Words'}, inplace = True)\n",
    "\n",
    "# plot the figure\n",
    "my_figure = px.line(results_melt, x = 'Date' , y = 'Frequency' , color = 'Words')\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by.svg)](https://creativecommons.org/licenses/by/4.0/legalcode)\n",
    "Esta lección tiene licencia bajo [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode) 2020 a Jeffrey C. Oliver."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
